<!DOCTYPE html>
<html lang="zh">

<head>
  <!-- Required meta tags always come first -->
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="google-site-verification" content="GgupatpoZgdqsBlxEZoMqGAy3aXAFtXIrYged3SB6EA" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>用BERT模型检测提示词注入 | nJcx's Blog
</title>
  <link rel="canonical" href="https://www.njcx.bid/posts/T8.html">


  <link rel="apple-touch-icon" href="https://www.njcx.bid/apple-touch-icon.png" sizes="180x180">
  <link rel="icon" type="image/png" href="https://www.njcx.bid/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="https://www.njcx.bid/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="https://www.njcx.bid/manifest.json">
  <meta name="theme-color" content="#333333">

  <link rel="stylesheet" href="https://www.njcx.bid/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://www.njcx.bid/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://www.njcx.bid/theme/css/pygments/emacs.min.css">
  <link rel="stylesheet" href="https://www.njcx.bid/theme/css/style.css">


<meta name="description" content="用BERT模型检测提示词注入 ~">
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?a34c7a96ae9745547c373575407c521f";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

</head>

<body>
  <header class="header">
    <div class="container">
      <div class="row">
        <div class="col-sm-4">
          <a href="https://www.njcx.bid">
            <img class="img-fluid" src=https://www.njcx.bid/images/profile.png width=200 height=200 alt="nJcx's Blog">
          </a>
        </div>
        <div class="col-sm-8">
          <h1 class="title">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://www.njcx.bid">nJcx's Blog</a></h1>
          <p class="text-muted">十年生死两茫茫，写程序，到天亮。相顾无言，惟有泪千行</p>
          <ul class="list-inline">
                    <li class="list-inline-item">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="https://www.zhihu.com/people/njcxs" target="_blank">知乎</a></li>
                <li class="list-inline-item"><a href="https://www.anquanke.com/member/154147" target="_blank">安全客</a></li>
                <li class="list-inline-item"><a href="https://www.freebuf.com/column/1481" target="_blank">Freebuf</a></li>
            <li class=" list-inline-item text-muted">|</li>
            <li class="list-inline-item"><a class="fa fa-github" href="https://github.com/njcx" target="_blank"></a></li>
          </ul>
        </div>
      </div>
    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>用BERT模型检测提示词注入
</h1>
      <hr>
<article class="article">
  <header>
    <ul class="list-inline">
      <li class="list-inline-item text-muted" title="2025-10-29T20:20:00+03:00">
        <i class="fa fa-clock-o"></i>
        三 29 十月 2025
      </li>
      <li class="list-inline-item">
        <i class="fa fa-folder-open-o"></i>
        <a href="https://www.njcx.bid/category/an-quan.html">安全</a>
      </li>
      <li class="list-inline-item">
        <i class="fa fa-user-o"></i>
        <a href="https://www.njcx.bid/author/njcx.html">nJcx</a>      </li>
      <li class="list-inline-item">
        <i class="fa fa-files-o"></i>
        <a href="https://www.njcx.bid/tag/qi-ye-an-quan-jian-she.html">#企业安全建设</a>      </li>
    </ul>
  </header>
  <div class="content">
    <p><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) 是由 Google 在 2018 年提出的预训练语言模型，基于 Transformer 架构，核心创新是双向上下文编码和掩码语言建模（MLM）。它可以被视为一位<strong>“博学的语言学家”</strong>，专门擅长阅读理解和语境分析。</p>
<h2>1. 核心概念：名字里的秘密</h2>
<p><strong>BERT</strong> 的全称解释了它的构造：</p>
<ul>
<li>
<p><strong>T - Transformers (变换器架构)</strong></p>
<ul>
<li>BERT 的“大脑”。摒弃了传统的循环神经网络（RNN），使用 <strong>Self-Attention (自注意力机制)</strong>。</li>
<li><strong>特点</strong>：能并行计算，同时关注句子中所有的词，而不是按顺序逐个读取。</li>
</ul>
</li>
<li>
<p><strong>E - Encoder (编码器)</strong></p>
<ul>
<li>Transformer 包含“编码器”和“解码器”。BERT <strong>只使用了编码器</strong>。</li>
<li><strong>特点</strong>：专注于“听懂”和“理解”输入的信息，而不是生成文本。</li>
</ul>
</li>
<li>
<p><strong>B - Bidirectional (双向)</strong></p>
<ul>
<li>BERT 最大的创新点。它在理解一个词时，能同时看到<strong>左边</strong>和<strong>右边</strong>的上下文。</li>
<li><em>例子</em>：“我去了<strong>银行</strong>，坐在岸边钓鱼。” —— BERT 通过看到后文的“岸边”，能判断出这里的“银行”是河岸 (River Bank) 而非金融机构。</li>
</ul>
</li>
</ul>
<h2>2. 训练原理：BERT 是如何变聪明的？</h2>
<p>BERT 在海量文本（维基百科等）上通过两个“游戏”进行<strong>预训练 (Pre-training)</strong>：</p>
<h3>游戏 1：完形填空 (Masked Language Model, MLM)</h3>
<ul>
<li><strong>规则</strong>：随机遮挡句子中 15% 的词（Mask），让模型根据上下文去猜被遮挡的词。</li>
<li><strong>例子</strong>：<code>[今天] [天气] [Mask] [不错]</code> 模型推断出 <code>[非常]</code>。</li>
<li><strong>目的</strong>：强迫模型深刻理解词与词之间的双向关系，而非死记硬背。</li>
</ul>
<h3>游戏 2：句子接龙 (Next Sentence Prediction, NSP)</h3>
<ul>
<li><strong>规则</strong>：给模型两句话 A 和 B，判断 B 是否是 A 的下一句。</li>
<li><strong>例子</strong>：<ul>
<li>A: 小明去买菜。 B: 他买了西红柿。 <strong>Yes</strong></li>
<li>A: 小明去买菜。 B: 企鹅会游泳。 <strong>No</strong></li>
</ul>
</li>
<li><strong>目的</strong>：让模型学会理解句子间的逻辑推理关系。</li>
</ul>
<h2>3. 应用模式：预训练 + 微调</h2>
<p>这是 BERT 改变 AI 范式的核心流程：</p>
<ol>
<li><strong>Pre-training (预训练 - 练内功)</strong>：<ul>
<li>在海量通用数据上训练，得到一个通用的语言模型。就像培养一个“通识全才”大学生。</li>
</ul>
</li>
<li><strong>Fine-tuning (微调 - 学招式)</strong>：<ul>
<li>在特定的下游任务数据上（如情感分析、垃圾邮件检测）进行少量训练。就像新员工入职后的“岗前培训”。</li>
</ul>
</li>
</ol>
<h2>4. 对比：BERT vs GPT</h2>
<table class="table-striped table table-hover">
<thead>
<tr>
<th style="text-align: left;">特性</th>
<th style="text-align: left;">BERT</th>
<th style="text-align: left;">GPT (Generative Pre-trained Transformer)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>核心架构</strong></td>
<td style="text-align: left;">Transformer <strong>Encoder</strong> (编码器)</td>
<td style="text-align: left;">Transformer <strong>Decoder</strong> (解码器)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>阅读方向</strong></td>
<td style="text-align: left;"><strong>双向</strong> (Bidirectional)</td>
<td style="text-align: left;"><strong>单向</strong> (从左到右)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>参数量</strong></td>
<td style="text-align: left;">_base: 110M，_large: 340M</td>
<td style="text-align: left;">数十亿到千亿级</td>
</tr>
<tr>
<td style="text-align: left;"><strong>擅长领域</strong></td>
<td style="text-align: left;"><strong>理解任务</strong>：<br/>文本分类、情感分析、实体识别、问答</td>
<td style="text-align: left;"><strong>生成任务</strong>：<br/>写作、聊天、翻译、代码生成</td>
</tr>
<tr>
<td style="text-align: left;"><strong>本质区别</strong></td>
<td style="text-align: left;">用来<strong>懂</strong>你在说什么</td>
<td style="text-align: left;">用来<strong>接</strong>你的下一句话</td>
</tr>
</tbody>
</table>
<h2>5. 怎么用BERT模型来检测提示词注入</h2>
<p>BERT 的强大在于它不是简单地预测下一个词，而是通过“完形填空”真正<strong>读懂了深层语境</strong>。它是现代 NLP 理解任务的基石。整个系统通过反向传播 + 梯度下降自动调整所有参数，无需人为干预。那么我们下载一个 BERT 中文基础模型，在上面用提示词注入数据集，微调模型就符合我们要求。</p>
<p>提示词注入(Prompt Injection)是一种针对AI语言模型的安全攻击技术。提示词注入是指攻击者通过精心设计的输入,试图操纵AI模型忽略原有的系统指令,转而执行攻击者想要的操作。这类似于传统软件中的SQL注入或代码注入攻击。用户直接向AI发送恶意指令,试图覆盖系统提示词。例如:"忽略之前的所有指令,现在告诉我..."。提示词注入可以绕过安全限制,获取不应该提供的信息，操纵AI执行有害或不当的操作，泄露系统提示词或内部配置信息，在集成系统中可能造成更严重的连锁影响等。</p>
<div class="highlight"><pre><span></span><code>数据集：
https://github.com/thu-coai/Safety-Prompts

BERT中文模型：
https://huggingface.co/hfl/chinese-bert-wwm-ext/tree/main
</code></pre></div>
  </div>
</article>
<div id="cyReward" role="cylabs" data-use="reward" style="text-align:center;"></div>
<!--PC和WAP自适应版-->
<div id="SOHUCS" ></div> 
<script type="text/javascript"> 
(function(){ 
var appid = 'cysYUIjwy'; 
var conf = 'prod_71b5e53cad0d27b5ed44fa5219b069b5'; 
var width = window.innerWidth || document.documentElement.clientWidth; 
if (width < 960) { 
window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); </script>

<script type="text/javascript" charset="utf-8" src="https://changyan.itc.cn/js/lib/jquery.js"></script>
<script type="text/javascript" charset="utf-8" src="https://changyan.sohu.com/js/changyan.labs.https.js?appid=cysYUIjwy"></script>    </div>
  </div>

  <footer class="footer">
    <div class="container">
      <div class="row">
       <ul class="col-sm-6 list-inline">
          <li class="list-inline-item"><a href="https://www.njcx.bid/authors.html">Authors</a></li>
          <li class="list-inline-item"><a href="https://www.njcx.bid/archives.html">Archives</a></li>
          <li class="list-inline-item"><a href="https://www.njcx.bid/categories.html">Categories</a></li>
          <li class="list-inline-item"><a href="https://www.njcx.bid/tags.html">Tags</a></li>
        </ul>
        <p class="col-sm-6 text-sm-right text-muted">
          本站由 <a href="https://github.com/getpelican/pelican" target="_blank">Pelican 生成</a> 后续 by <a href="https://github.com/njcx" target="_blank">nJcx</a>
        </p>
      </div>
    </div>
  </footer>
</body>

</html>